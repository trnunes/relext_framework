#permitir varias configs identificando-as atrav�s de ID
minning:
  enabled: 'no'
  target_relations: [ceo, chairperson, secretaryGeneral]
  uri_pattern: "http://dbpedia.org/ontology/"
  max_examples_quantity: 115
  #separar os m�dulos por diret�rio
  preprocessing:
    tokenizer: DefaultTokenizer
    text_cleaner: WikipediaTextCleaner
    #text_cleaner: DefaultTextCleaner
    radical_extractor: DefaultRadicalExtractor
    sentence_extractor: OpenNLPSentenceExtractor
  filters: [WikiLinkFilter]
  db_config:
    db: mysql
    location: "jdbc:mysql://localhost:3306/unclassified_sentences_db?user=root&password=db@dm348"
  relations_db_config:
    db: sparql
    location: "http://live.dbpedia.org/sparql"
    
dataset_generation:
  enabled: 'yes'
  target_relations: [
musicalBand,
producer,
isPartOf,
college,
writer,
nearestCity,
region,
award,
family,
riverMouth,
district]
  max_examples_quantity: 500
  uri_pattern: "http://dbpedia.org/ontology/"
  preprocessing:
    tokenizer: DefaultTokenizer
    text_cleaner: WikipediaTextCleaner
    radical_extractor: DefaultRadicalExtractor
    sentence_extractor: OpenNLPSentenceExtractor
  filters: []
  feature_extractors: [LexicalExtractor, ShortestPathExtractor, EntityTypesExtractor]
  dataset_format: arff
  dataset_name: en_500_full
  dataset_folder: ./trainning_data
  db_config:
    db: mysql
    location: jdbc:mysql://139.82.71.42:3306/my_dataset?user=root&password=1234
trainning:
  enabled: 'no'
  dataset_folder: ./trainning_data
  dataset_name: shortest_path
  dataset_format: csv
  classifier_folder: ./ml_models
  classifier: WekaNaiveBayes
  cross_validation_folds: 10
  classifier_settings: ''
  
nlp:
  parser: StanfordDependencyParser
  grammar: lib/stanford-parser-2012-03-09/englishPCFG.ser.gz
  
information_extraction:
  correference_analyser: WikipediaCorreferenceAnalyser
  corpus: 
    class: WikipediaCorpus
    url: http://localhost/mediawiki/index.php
    lang: en